{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework â„–2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost Shakespeare\n",
    "\n",
    "Let's try to generate some Shakespeare poetry using RNNs. The sonnets file is available in the notebook directory.\n",
    "\n",
    "Text generation can be designed in several steps:\n",
    "    \n",
    "1. Data loading.\n",
    "2. Dictionary generation.\n",
    "3. Data preprocessing.\n",
    "4. Model (neural network) training.\n",
    "5. Text generation (model evaluation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`).\n",
    "\n",
    "Simple preprocessing is already done for you in the next cell: all technical info is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "import string\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('../../datasets/Shakespeare_sonnets/sonnets.txt', 'r') as iofile:\n",
    "        text = iofile.readlines()\n",
    "except FileNotFoundError:\n",
    "    !wget https://raw.githubusercontent.com/neychev/made_nlp_course/master/datasets/Shakespeare_sonnets/sonnets.txt -nc\n",
    "    with open('sonnets.txt', 'r') as iofile:\n",
    "        text = iofile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_START = 45\n",
    "TEXT_END = -368\n",
    "text = text[TEXT_START : TEXT_END]\n",
    "assert len(text) == 2616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
    "\n",
    "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "# Join all the strings into one and lowercase it\n",
    "# Put result into variable text.\n",
    "\n",
    "# Your great code here\n",
    "text =  ''.join(text).lower()\n",
    "\n",
    "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
    "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the characters, that you've seen in the text, into variable `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.replace('\\r\\n', ' ')\n",
    "# text = text.replace('\\n', ' ')\n",
    "# text = text.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict <index>:<char>\n",
    "# Your great code here\n",
    "token_to_id = {\n",
    "    token: idx for idx, token in enumerate(tokens)\n",
    "}\n",
    "# dict <char>:<index>\n",
    "# Your great code here\n",
    "idx_to_token = {idx: word for word, idx in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(tokens)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([token_to_id[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded) / (100 * 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 32\n",
    "batch_size = 32\n",
    "lstm_size=38\n",
    "embedding_size=38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(encoded) / (seq_size * batch_size))\n",
    "in_text = encoded[:num_batches * batch_size * seq_size]\n",
    "out_text = np.zeros_like(in_text)\n",
    "out_text[:-1] = in_text[1:]\n",
    "out_text[-1] = in_text[0]\n",
    "in_text = np.reshape(in_text, (batch_size, -1))\n",
    "out_text = np.reshape(out_text, (batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100225,), 99328)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape, num_batches * batch_size * seq_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 3104), (32, 3104), 97)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_text.shape, out_text.shape, np.prod(in_text.shape) // (seq_size * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 17, 29, ..., 26, 29,  1],\n",
       "       [25, 26,  1, ..., 14, 19,  1],\n",
       "       [31, 26,  1, ..., 31, 16, 31],\n",
       "       ...,\n",
       "       [ 1, 16, 36, ..., 20, 25, 15],\n",
       "       [10,  0,  1, ..., 16, 12, 33],\n",
       "       [16, 25,  1, ..., 36, 16,  1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text[0:0+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(n_vocab,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, num_tokens)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state        \n",
    "        \n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModule(num_tokens, seq_size, embedding_size, lstm_size)\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_words=['I', 'am'],\n",
    "def predict(net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[token_to_id[w]]])\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]])\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 Iteration: 100 Loss: 1.9121330976486206\n",
      "Epoch: 2/200 Iteration: 200 Loss: 1.878706693649292\n",
      "Epoch: 3/200 Iteration: 300 Loss: 1.8017863035202026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0bc2bc1e52b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-28836573e473>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, prev_state)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 559\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "best_loss = np.inf\n",
    "for e in range(200):\n",
    "    batches = get_batches(in_text, out_text, batch_size, seq_size)\n",
    "    state_h, state_c = net.zero_state(batch_size)\n",
    "\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        # Tell it we are in training mode\n",
    "        net.train()\n",
    "\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Transfer data to GPU\n",
    "        x = torch.tensor(x).to(torch.int64)\n",
    "        y = torch.tensor(y).to(torch.int64)\n",
    "\n",
    "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "        state_h = state_h.detach()\n",
    "        state_c = state_c.detach()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Perform back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        _ = torch.nn.utils.clip_grad_norm_(\n",
    "            net.parameters(), 5)\n",
    "\n",
    "        optimizer.step()\n",
    "        if iteration % 100 == 0:\n",
    "            print('Epoch: {}/{}'.format(e, 200),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "\n",
    "#             if iteration % 1000 == 0:\n",
    "    if loss_value < best_loss:\n",
    "#         predict(net, initial_words, num_tokens,\n",
    "#                 token_to_id, idx_to_token, top_k=5)\n",
    "        torch.save(net.state_dict(),\n",
    "                   'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "            \n",
    "    epoch_loss.append(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
    "\n",
    "Let's use vanilla RNN, similar to the one created during the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(names, max_len=None, pad=token_to_id[' '], dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, names))\n",
    "    names_ix = np.zeros([len(names), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        line_ix = [token_to_id[c] for c in names[i]]\n",
    "        names_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        names_ix = np.transpose(names_ix)\n",
    "\n",
    "    return names_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      "fair\n",
      "and\n",
      "by\n",
      "being\n",
      "sleep\n",
      "beloved\n",
      "retention\n",
      "foes,\n",
      "[[15 27 24 22  0  0  0  0  0]\n",
      " [15 10 18 27  0  0  0  0  0]\n",
      " [10 23 13  0  0  0  0  0  0]\n",
      " [11 34  0  0  0  0  0  0  0]\n",
      " [11 14 18 23 16  0  0  0  0]\n",
      " [28 21 14 14 25  0  0  0  0]\n",
      " [11 14 21 24 31 14 13  0  0]\n",
      " [27 14 29 14 23 29 18 24 23]\n",
      " [15 24 14 28  5  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(words[::2000]))\n",
    "print(to_matrix(words[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNNCell(nn.Module):\n",
    "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
    "        super(self.__class__,self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # get vector embedding of x\n",
    "        x_emb = self.embedding(x)\n",
    "        \n",
    "        # compute next hidden state using self.rnn_update\n",
    "        # hint: use torch.cat(..., dim=...) for concatenation\n",
    "        x_and_h = torch.cat([x_emb, h_prev], dim=1) # YOUR CODE HERE\n",
    "        h_next = self.rnn_update(x_and_h) # YOUR CODE HERE    \n",
    "        h_next = torch.tanh(h_next) # YOUR CODE HERE\n",
    "        assert h_next.size() == h_prev.size()\n",
    "        \n",
    "        #compute logits for next character probs\n",
    "        logits = self.rnn_to_logits(h_next)# YOUR CODE\n",
    "        \n",
    "        return h_next, F.log_softmax(logits, -1)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_units, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn = CharRNNCell()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_loop(char_rnn, batch_ix):\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = char_rnn.initial_state(batch_size)\n",
    "    logprobs = []\n",
    "\n",
    "    for x_t in batch_ix.transpose(0,1):\n",
    "        hid_state, logp_next = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
    "        logprobs.append(logp_next)\n",
    "        \n",
    "    return torch.stack(logprobs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ix = to_matrix(words[:5])\n",
    "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "logp_seq = rnn_loop(char_rnn, batch_ix)\n",
    "\n",
    "assert torch.max(logp_seq).data.numpy() <= 0\n",
    "assert tuple(logp_seq.size()) ==  batch_ix.shape + (num_tokens,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logp = logp_seq[:, :-1]\n",
    "actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "loss = criterion(predictions_logp.contiguous().view(-1, num_tokens), \n",
    "                  actual_next_tokens.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 36])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_logp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_next_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in char_rnn.parameters():\n",
    "    assert w.grad is not None and torch.max(torch.abs(w.grad)).data.numpy() != 0, \\\n",
    "        \"Loss is not differentiable w.r.t. a weight with shape %s. Check forward method.\" % (w.size(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5736, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_rnn = CharRNNCell()\n",
    "criterion = nn.NLLLoss()\n",
    "opt = torch.optim.Adam(char_rnn.parameters())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length = 14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(len, words))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, words)),bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555.4375"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words) / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = 16\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_ix = to_matrix(sample(words, 32), max_len=MAX_LENGTH)\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "    \n",
    "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = logp_seq[:, :-1]# YOUR CODE HERE\n",
    "    actual_next_tokens = batch_ix[:, 1:]# YOUR CODE HERE\n",
    "\n",
    "    loss = criterion(\n",
    "        predictions_logp.contiguous().view(-1, num_tokens),\n",
    "        actual_next_tokens.contiguous().view(-1)\n",
    "    ) # YOUR CODE HERE\n",
    "    \n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' Hello', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs, \n",
    "        smaller temperature converges to the single most likely output.\n",
    "        \n",
    "    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)\n",
    "    of the next symbol.\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([[x_sequence]], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        print(x_sequence[:, -1].shape, hid_state.shape)\n",
    "        out, hid_state = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        print(x_sequence.shape, x_sequence, hid_state.shape)\n",
    "        out, hid_state = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        # Be really careful here with the model output\n",
    "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        print(p_next.shape, len(tokens))\n",
    "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        print(x_sequence.shape, next_ix.shape)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(num_tokens,p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with                \n"
     ]
    }
   ],
   "source": [
    "# An example of generated text.\n",
    "# print(generate_text(length=500, temperature=0.2))\n",
    "print(generate_sample(char_rnn, seed_phrase = 'wi', max_length=20, temperature=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More poetic model\n",
    "\n",
    "Let's use LSTM instead of vanilla RNN and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function of the number of epochs. Does the final loss become better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
    "\n",
    "Evaluate the results visually, try to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with different temperature values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
    "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
    "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
    "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
